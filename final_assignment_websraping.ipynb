tiveclass.ai logo
Extracting Stock Data Using a Web Scraping
Not all stock data is available via API in this assignment; you will use web-scraping to obtain financial data. You will be quizzed on your results.
Using beautiful soup we will extract historical share data from a web-page.

Table of Contents
Downloading the Webpage Using Requests Library
Parsing Webpage HTML Using BeautifulSoup
Extracting Data and Building DataFrame
Estimated Time Needed: 30 min

#!pip install pandas
#!pip install requests
!pip install bs4
#!pip install plotly
Collecting bs4
  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz
Collecting beautifulsoup4 (from bs4)
  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)
     |████████████████████████████████| 122kB 26.8MB/s eta 0:00:01
Collecting soupsieve>1.2; python_version >= "3.0" (from beautifulsoup4->bs4)
  Downloading https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl
Building wheels for collected packages: bs4
  Building wheel for bs4 (setup.py) ... done
  Stored in directory: /home/jupyterlab/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472
Successfully built bs4
Installing collected packages: soupsieve, beautifulsoup4, bs4
Successfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.2.1
import pandas as pd
import requests
from bs4 import BeautifulSoup
Using Webscraping to Extract Stock Data Example
First we must use the request library to downlaod the webpage, and extract the text. We will extract Netflix stock data https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html.

url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html"

data  = requests.get(url).text
Next we must parse the text into html using beautiful_soup

soup = BeautifulSoup(data, 'html5lib')
Now we can turn the html table into a pandas dataframe

netflix_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Volume"])

# First we isolate the body of the table which contains all the information
# Then we loop through each row and find all the column values for each row
for row in soup.find("tbody").find_all('tr'):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text
    
    # Finally we append the data of each row to the table
    netflix_data = netflix_data.append({"Date":date, "Open":Open, "High":high, "Low":low, "Close":close, "Adj Close":adj_close, "Volume":volume}, ignore_index=True)    
We can now print out the dataframe

netflix_data.head()
Date	Open	High	Low	Close	Volume	Adj Close
0	Jun 01, 2021	504.01	536.13	482.14	528.21	78,560,600	528.21
1	May 01, 2021	512.65	518.95	478.54	502.81	66,927,600	502.81
2	Apr 01, 2021	529.93	563.56	499.00	513.47	111,573,300	513.47
3	Mar 01, 2021	545.57	556.99	492.85	521.66	90,183,900	521.66
4	Feb 01, 2021	536.79	566.65	518.28	538.85	61,902,300	538.85
We can also use the pandas read_html function using the url

read_html_pandas_data = pd.read_html(url)
Or we can convert the BeautifulSoup object to a string

read_html_pandas_data = pd.read_html(str(soup))
Beacause there is only one table on the page, we just take the first table in the list returned

netflix_dataframe = read_html_pandas_data[0]

netflix_dataframe.head()
Date	Open	High	Low	Close*	Adj Close**	Volume
0	Jun 01, 2021	504.01	536.13	482.14	528.21	528.21	78560600
1	May 01, 2021	512.65	518.95	478.54	502.81	502.81	66927600
2	Apr 01, 2021	529.93	563.56	499.00	513.47	513.47	111573300
3	Mar 01, 2021	545.57	556.99	492.85	521.66	521.66	90183900
4	Feb 01, 2021	536.79	566.65	518.28	538.85	538.85	61902300
Using Webscraping to Extract Stock Data Exercise
Use the requests library to download the webpage https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html. Save the text of the response as a variable named html_data.

url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html"
html_data = requests.get(url).text
Parse the html data using beautiful_soup.

soup = BeautifulSoup(html_data, 'html5lib')
Question 1 What is the content of the title attribute:

soup.title.text
'Amazon.com, Inc. (AMZN) Stock Historical Prices & Data - Yahoo Finance'
Using beautiful soup extract the table with historical share prices and store it into a dataframe named amazon_data. The dataframe should have columns Date, Open, High, Low, Close, Adj Close, and Volume. Fill in each variable with the correct data from the list col.

amazon_data = pd.DataFrame(columns=["Date", "Open", "High", "Low", "Close", "Adj Close" , "Volume"])

for row in soup.find("tbody").find_all("tr"):
    col = row.find_all("td")
    date = col[0].text
    Open = col[1].text
    high = col[2].text
    low = col[3].text
    close = col[4].text
    adj_close = col[5].text
    volume = col[6].text
    
    amazon_data = amazon_data.append({"Date":date, "Open":Open, "High":high, "Low":low, "Close":close, "Adj Close":adj_close, "Volume":volume}, ignore_index=True)
Print out the first five rows of the amazon_data dataframe you created.

amazon_data.head(5)
Date	Open	High	Low	Close	Adj Close	Volume
0	Jan 01, 2021	3,270.00	3,363.89	3,086.00	3,206.20	3,206.20	71,528,900
1	Dec 01, 2020	3,188.50	3,350.65	3,072.82	3,256.93	3,256.93	77,556,200
2	Nov 01, 2020	3,061.74	3,366.80	2,950.12	3,168.04	3,168.04	90,810,500
3	Oct 01, 2020	3,208.00	3,496.24	3,019.00	3,036.15	3,036.15	116,226,100
4	Sep 01, 2020	3,489.58	3,552.25	2,871.00	3,148.73	3,148.73	115,899,300
Question 2 What is the name of the columns of the dataframe

list(amazon_data.columns)
['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
Question 3 What is the Open of the last row of the amazon_data dataframe?

amazon_data["Open"][60]
'656.29'
About the Authors:
Joseph Santarcangelo has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.

Azim Hirjani

Change Log
Date (YYYY-MM-DD)	Version	Changed By	Change Description
| 2021-06-09       | 1.2     | Lakshmi Holla|Added URL in question 3 |
| 2020-11-10 | 1.1 | Malika Singla | Deleted the Optional part | | 2020-08-27 | 1.0 | Malika Singla | Added lab to GitLab |

© IBM Corporation 2020. All rights reserved.
